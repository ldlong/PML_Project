1100 + c(-1,1)*qt(.975,8)*30/sqrt(9)
library(AppliedPredictiveModeling);
library(caret);
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
describe(training)
summary(training)
str(training)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training$Superplasticizer, breaks=20)
IL_col_idx <- grep("^[Ii][Ll].*", names(training))
preObj <- preProcess(training[, IL_col_idx], method=c("center", "scale", "pca"), thresh=0.9)
library(lattice)
library(gplots)
lambda <- 0.2
n <- 40
simulations <- 1:1000
set.seed(100)
means <- data.frame(x = sapply(simulations, function(x) {mean(rexp(n, lambda))}))
```
```{r}
mean(means$x) # sampling mean = 5.000542
(1/lambda) # theoretical mean = 5
```{r}
library(lattice)
library(gplots)
lambda <- 0.2
n <- 40
simulations <- 1:1000
set.seed(100)
means <- data.frame(x = sapply(simulations, function(x) {mean(rexp(n, lambda))}))
```
```{r}
mean(means$x) # sampling mean = 5.000542
(1/lambda) # theoretical mean = 5
```
means
mean(means)
mean(means$x)
(1/lambda)
The sampling mean is **`r format(sample_mean, digits = 4)`** which is similar to the theoretical mean of **`r format(theo_mean, digits = 4)`**.
The Central Limit Theorem holds for this simulation.
sample_mean = mean(means$x)
theo_mean = (1/lambda)
The sampling mean is **`r format(sample_mean, digits = 4)`** which is similar to the theoretical mean of **`r format(theo_mean, digits = 4)`**.
The Central Limit Theorem holds for this simulation.
```{r, echo=FALSE}
tmp <- hist(means$x, freq=TRUE, main="Data histogram and normal curve",
col="lightgreen", axes = FALSE, xlab='Mean', yaxt='n', ylab='Frequency')
axis(2, pos = 2.5)
axis(1,at=seq(0,8,1), pos=0)
factor <- tmp$counts / tmp$density
samp_density <- density(means$x)
samp_density$y <- samp_density$y * factor[1]
samp_x <- seq(2.5, 8, length.out= 100)
normal <- dnorm(x = samp_x, mean = samp_mean, sd = samp_sd)
lines(samp_x, normal * factor[1], col = "blue", lwd = 3)
normal <- dnorm(x = samp_x, mean = samp_mean, sd = samp_sd)
samp_mean = mean(means$x)
theo_mean = (1/lambda)
tmp <- hist(means$x, freq=TRUE, main="Histogram and Normal Curve",
col="red", axes = FALSE, xlab='Mean', yaxt='n', ylab='Frequency')
axis(2, pos = 2.5)
axis(1,at=seq(0,8,1), pos=0)
factor <- tmp$counts / tmp$density
samp_density <- density(means$x)
samp_density$y <- samp_density$y * factor[1]
samp_x <- seq(2.5, 8, length.out= 100)
normal <- dnorm(x = samp_x, mean = samp_mean, sd = samp_sd)
lines(samp_x, normal * factor[1], col = "black", lwd = 3)
samp_var = var(means$x)
samp_sd = sd(means$x)
theo_var = ((1/lambda)/sqrt(n))^2
theo_sd = (1/lambda)/sqrt(n)
tmp <- hist(means$x, freq=TRUE, main="Histogram and Normal Curve",
col="red", axes = FALSE, xlab='Mean', yaxt='n', ylab='Frequency')
axis(2, pos = 2.5)
axis(1,at=seq(0,8,1), pos=0)
factor <- tmp$counts / tmp$density
samp_density <- density(means$x)
samp_density$y <- samp_density$y * factor[1]
samp_x <- seq(2.5, 8, length.out= 100)
normal <- dnorm(x = samp_x, mean = samp_mean, sd = samp_sd)
lines(samp_x, normal * factor[1], col = "black", lwd = 3)
tmp <- hist(means$x, freq=TRUE, main="Histogram and Normal Curve",
col="lightblue", axes = FALSE, xlab='Mean', yaxt='n', ylab='Frequency')
axis(2, pos = 2.5)
axis(1,at=seq(0,8,1), pos=0)
factor <- tmp$counts / tmp$density
samp_density <- density(means$x)
samp_density$y <- samp_density$y * factor[1]
samp_x <- seq(2.5, 8, length.out= 100)
normal <- dnorm(x = samp_x, mean = samp_mean, sd = samp_sd)
lines(samp_x, normal * factor[1], col = "black", lwd = 3)
ToothGrowth$dose
ToothGrowth$dose = factor(ToothGrowth$dose, levels=c(0.5,1.0,2.0),
labels=c("low","med","high"))
table(supp,dose)
attach(ToothGrowth)
table(supp,dose)
table(supp,dose)
ToothGrowth$dose
ToothGrowth$dose = factor(ToothGrowth$dose, levels=c(0.5,1.0,2.0),
labels=c("low","med","high"))
table(supp,dose)
```{r, results='hide'}
library(datasets)
data(mtcars)
str(mtcars)
library(lattice)
library(gplots)
bwplot(mpg ~ am)
library(datasets)
data(mtcars)
str(mtcars)
library(lattice)
library(gplots)
attach(mtcars)
bwplot(mpg ~ am)
plot(mpg ~ am)
summary(mtcars$mpg)
aggregate(mpg,am, mean)
aggregate(mpg,am, sd)
summary(mtcars$mpg)
aggregate(mpg,list(am), mean)
aggregate(len,list(am), sd)
summary(mtcars$mpg)
aggregate(mpg,list(am), mean)
aggregate(mpg,list(am), sd)
lm <- lm(mpg ~ am, data = mtcars)
summary(lm)
help (mtcars)
mtcars$am = factor(mtcars$am, levels=c(0, 1),
labels=c("Automatic","Manual"))
attach(mtcars)
plot(mpg ~ am)
mtcars$am = factor(mtcars$am, levels=c(0, 1),
labels=c("Automatic","Manual"))
attach(mtcars)
bwplot(mpg ~ am)
mtcars$am = factor(mtcars$am, levels=c(0, 1),
labels=c("Automatic","Manual"))
attach(mtcars)
plot(mpg ~ am)
mtcars$am = factor(mtcars$am, levels=c(0, 1),
labels=c("Automatic","Manual"))
attach(mtcars)
plot(mpg ~ am)
data(mtcars)
library(datasets)
data(mtcars)
str(mtcars)
mtcars$am <- as.factor(mtcars$am)
par(mfrow=c(3,2))
par(mar=c(2.5, 5.5, 1.5, 1.5))
boxplot(mpg ~ am, data = mtcars, xlab = "AM (Transmission type)",
ylab = "MPG (Miles per galon)", main="Boxplot", xaxt="n", col=c("red","blue"))
axis(1, at=c(1,2), labels=c("automatic", "manual"))
par(mar=c(2.5, 5.5, 1.5, 1.5))
plot(lm)
par(mfrow=c(2,3))
par(mar=c(2.5, 5.5, 1.5, 1.5))
boxplot(mpg ~ am, data = mtcars, xlab = "Transmission type",
ylab = "MPG (Miles per galon)", main="Boxplot", xaxt="n", col=c("red","blue"))
axis(1, at=c(1,2), labels=c("automatic", "manual"))
par(mar=c(2.5, 5.5, 1.5, 1.5))
plot(lm)
lm <- lm(mpg ~ am + cyl, data = mtcars)
summary(lm)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_col_idx <- grep("^[Ii][Ll].*", names(training))
preObj <- preProcess(training[, IL_col_idx], method=c("center", "scale", "pca"), thresh=0.8)
preObj
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_col_idx <- grep("^[Ii][Ll].*", names(training))
preObj <- preProcess(training[, IL_col_idx], method=c("center", "scale", "pca"), thresh=0.9)
preObj
preObj <- preProcess(training[, IL_col_idx], method=c("center", "scale", "pca"), thresh=0.8)
preObj
setup_twitter_oauth("TZsAKDzO1pn8U1n2RNvzANTQU","iD4fGlXrD8Ygcty3PePoScpGQIG1zHjj7w6Fnkp32MLlEzseR5")
library(twitteR)
library(twitteR)
setup_twitter_oauth("TZsAKDzO1pn8U1n2RNvzANTQU","iD4fGlXrD8Ygcty3PePoScpGQIG1zHjj7w6Fnkp32MLlEzseR5")
setup_twitter_oauth(TZsAKDzO1pn8U1n2RNvzANTQU,iD4fGlXrD8Ygcty3PePoScpGQIG1zHjj7w6Fnkp32MLlEzseR5)
settup_twitter_oauth("TZsAKDzO1pn8U1n2RNvzANTQU","iD4fGlXrD8Ygcty3PePoScpGQIG1zHjj7w6Fnkp32MLlEzseR5","631776324-IeCyWOdVoTOgFzN0Hy3Fx44juzlNiG0UYbnGwiGR","yj6cDZKi0J4vIrznmu3EaOW7xwHn66wSmmdstv0Ltlc6I")
setup_twitter_oauth("TZsAKDzO1pn8U1n2RNvzANTQU","iD4fGlXrD8Ygcty3PePoScpGQIG1zHjj7w6Fnkp32MLlEzseR5","631776324-IeCyWOdVoTOgFzN0Hy3Fx44juzlNiG0UYbnGwiGR","yj6cDZKi0J4vIrznmu3EaOW7xwHn66wSmmdstv0Ltlc6I")
rdmTweets <- userTimeline("rdatamining", n=100)
n <- length(rdmTweets)
rdmTweets[1:3]
rdmTweets <- userTimeline("antoniolpee", n=100)
n <- length(rdmTweets)
rdmTweets[1:3]
# retrieve the first 100 tweets (or all tweets if fewer than 100)
# from the user timeline of @rdatammining
rdmTweets <- userTimeline("longlarryd", n=100)
n <- length(rdmTweets)
rdmTweets[1:3]
df <- do.call("rbind", lapply(rdmTweets, as.data.frame))
dim(df)
str(df)
## TRANSFORING TEXT
myCorpus <- tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove stopwords
# keep "r" by removing it from stopwords
myStopwords <- c(stopwords('english'), "available", "via")
idx <- which(myStopwords == "r")
myStopwords <- myStopwords[-idx]
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
library(tm)
myCorpus <- Corpus(VectorSource(df$text))
## TRANSFORING TEXT
myCorpus <- tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# remove stopwords
# keep "r" by removing it from stopwords
myStopwords <- c(stopwords('english'), "available", "via")
idx <- which(myStopwords == "r")
myStopwords <- myStopwords[-idx]
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## STEMMING WORDS
dictCorpus <- myCorpus
# stem words in a text document with the snowball stemmers,
# which requires packages Snowball, RWeka, rJava, RWekajars
myCorpus <- tm_map(myCorpus, stemDocument)
# inspect the first three ``documents"
inspect(myCorpus[1:3])
# stem completion
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
inspect(myCorpus[1:3])  #print the first three documents
## BUILDING A DOCUMENT TERM MATRIX
myDtm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
inspect(myDtm[266:270,31:40])
myDtm <- TermDocumentMatrix(myCorpus, control = list(minWordLength = 1))
myDtm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
myCorpus <- tm_map(myCorpus, PlainTextDocument) #convert corpus to plain text
myDtm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
inspect(myDtm[266:270,31:40])
findFreqTerms(myDtm, lowfreq=10)
findAssocs(myDtm, 'r', 0.30)
findAssocs(myDtm, 'students', 0.30)
library(wordcloud)
m <- as.matrix(myDtm)
# calculate the frequency of words
v <- sort(rowSums(m), decreasing=TRUE)
myNames <- names(v)
k <- which(names(v)=="miners")
myNames[k] <- "mining"
d <- data.frame(word=myNames, freq=v)
wordcloud(d$word, d$freq, min.freq=3)
dictCorpus <- myCorpus
# stem words in a text document with the snowball stemmers,
# which requires packages Snowball, RWeka, rJava, RWekajars
myCorpus <- tm_map(myCorpus, stemDocument)
inspect(myCorpus[1:3])
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=dictCorpus)
inspect(myCorpus[1:3])  #print the first three documents
myDtm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
inspect(myDtm[266:270,31:40])
## FREQUENT TERM ASSOCIATIONS
findFreqTerms(myDtm, lowfreq=10)
# which words are associated with "students"?
findAssocs(myDtm, 'students', 0.30)
## WORD CLOUD
library(wordcloud)
m <- as.matrix(myDtm)
# calculate the frequency of words
v <- sort(rowSums(m), decreasing=TRUE)
myNames <- names(v)
k <- which(names(v)=="miners")
myNames[k] <- "mining"
d <- data.frame(word=myNames, freq=v)
wordcloud(d$word, d$freq, min.freq=3)
d
v <- sort(rowSums(m), decreasing=TRUE)
myNames <- names(v)
#k <- which(names(v)=="miners")
#myNames[k] <- "mining"
d <- data.frame(word=myNames, freq=v)
wordcloud(d$word, d$freq, min.freq=3)
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
myDtm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
myCorpus <- tm_map(myCorpus, PlainTextDocument) #convert corpus to plain text
myDtm <- TermDocumentMatrix(myCorpus, control = list(wordLengths = c(1, Inf)))
m <- as.matrix(myDtm)
# calculate the frequency of words
v <- sort(rowSums(m), decreasing=TRUE)
myNames <- names(v)
#k <- which(names(v)=="miners")
#myNames[k] <- "mining"
d <- data.frame(word=myNames, freq=v)
wordcloud(d$word, d$freq, min.freq=3)
clear
source(“http://biostat.jhsph.edu/~jleek/code/twitterMap.R”)
twitterMap("antoniolpee", fileName=”twitterMap.pdf”, nMax=1500)
source("http://biostat.jhsph.edu/~jleek/code/twitterMap.R"")
twitterMap("antoniolpee", fileName="twitterMap.pdf", nMax=1500)
source("http://biostat.jhsph.edu/~jleek/code/twitterMap.R")
twitterMap("antoniolpee", fileName="twitterMap.pdf", nMax=1500)
twitterMap("tnsmith922", fileName="twitterMap2.pdf", nMax=1500)
twitterMap("longlarryd", fileName="twitterMap2.pdf", nMax=1500)
# load packages
library(twitteR)
library(igraph)
library(stringr)
setup_twitter_oauth("TZsAKDzO1pn8U1n2RNvzANTQU","iD4fGlXrD8Ygcty3PePoScpGQIG1zHjj7w6Fnkp32MLlEzseR5","631776324-IeCyWOdVoTOgFzN0Hy3Fx44juzlNiG0UYbnGwiGR","yj6cDZKi0J4vIrznmu3EaOW7xwHn66wSmmdstv0Ltlc6I")
mh370 <- searchTwitter("#PrayForMH370", since = "2014-03-08", until = "2014-03-20", n = 1000)
mh370_text = sapply(mh370, function(x) x$getText())
mh370_corpus = Corpus(VectorSource(mh370_text))
tdm = TermDocumentMatrix(
mh370_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c("prayformh370", "prayformh", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing = TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word = names(word_freqs), freq = word_freqs)
wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
library(twitteR)
library(tm)
library(wordcloud)
library(RColorBrewer)
setup_twitter_oauth("TZsAKDzO1pn8U1n2RNvzANTQU","iD4fGlXrD8Ygcty3PePoScpGQIG1zHjj7w6Fnkp32MLlEzseR5","631776324-IeCyWOdVoTOgFzN0Hy3Fx44juzlNiG0UYbnGwiGR","yj6cDZKi0J4vIrznmu3EaOW7xwHn66wSmmdstv0Ltlc6I")
mh370 <- searchTwitter("#PrayForMH370", since = "2014-03-08", until = "2014-03-20", n = 1000)
mh370_text = sapply(mh370, function(x) x$getText())
mh370_corpus = Corpus(VectorSource(mh370_text))
tdm = TermDocumentMatrix(
mh370_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c("prayformh370", "prayformh", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing = TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word = names(word_freqs), freq = word_freqs)
wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
mh370 <- searchTwitter("#brody", n = 1000)
mh370_text = sapply(mh370, function(x) x$getText())
mh370_corpus = Corpus(VectorSource(mh370_text))
tdm = TermDocumentMatrix(
mh370_corpus,
control = list(
removePunctuation = TRUE,
stopwords = c("prayformh370", "prayformh", stopwords("english")),
removeNumbers = TRUE, tolower = TRUE)
)
m = as.matrix(tdm)
# get word counts in decreasing order
word_freqs = sort(rowSums(m), decreasing = TRUE)
# create a data frame with words and their frequencies
dm = data.frame(word = names(word_freqs), freq = word_freqs)
wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, "Dark2"))
mh370_corpus
mh370_text
load <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE)
sapply(pkg, require, character.only = TRUE)
}
packages <- c("data.table", "caret", "randomForest", "foreach", "rpart", "rpart.plot", "corrplot")
load(packages)
setwd("C:\Users\ldl7766\Desktop\Coursera\A. Data Science Specialization\8. Practical Machine Learning\Project")
setwd("\Users\ldl7766\Desktop\Coursera\A. Data Science Specialization\8. Practical Machine Learning\Project")
setwd("C:/Users/ldl7766/Desktop/Coursera/A. Data Science Specialization/8. Practical Machine Learning/Project")
## Load Data
training_data <- read.csv("pml-training.csv", na.strings=c("#DIV/0!"," ", "", "NA", "NAs", "NULL"))
testing_data <- read.csv("pml-testing.csv", na.strings=c("#DIV/0!"," ", "", "NA", "NAs", "NULL"))
str(training_data)
cleantraining <- training_data[, -which(names(training_data) %in% c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window"))]
cleantraining = cleantraining[, colSums(is.na(cleantraining)) == 0] #this drops columns with NAs
zerovariance =nearZeroVar(cleantraining[sapply(cleantraining, is.numeric)], saveMetrics=TRUE)
cleantraining = cleantraining[, zerovariance[, 'nzv'] == 0] #to remove 0 or near to 0 variance variables
correlationmatrix <- cor(na.omit(cleantraining[sapply(cleantraining, is.numeric)]))
dim(correlationmatrix)
correlationmatrixdegrees
correlationmatrixdegreesoffreedom <- expand.grid(row = 1:52, col = 1:52)
correlationmatrixdegreesoffreedom$correlation <- as.vector(correlationmatrix) #this returns the correlation matrix in matrix format
removehighcorrelation <- findCorrelation(correlationmatrix, cutoff = .7, verbose = TRUE)
cleantraining <- cleantraining[, -removehighcorrelation] #this removes highly correlated variables (in psychometric theory .7+ correlation is a high correlation)
for(i in c(8:ncol(cleantraining)-1)) {cleantraining[,i] = as.numeric(as.character(cleantraining[,i]))}
for(i in c(8:ncol(testing_data)-1)) {testing_data[,i] = as.numeric(as.character(testing_data[,i]))} #Some columns were blank, hence are dropped. I will use a set that only includes complete columns. I also remove user name, timestamps and windows to have a light data set.
featureset <- colnames(cleantraining[colSums(is.na(cleantraining)) == 0])[-(1:7)]
modeldata <- cleantraining[featureset]
featureset #now we have the model data built from our feature set.
##Cross-Validation
#I need to split the sample in two samples. This is to divide training and testing for cross-validation.
idx <- createDataPartition(modeldata$classe, p=0.6, list=FALSE )
training <- modeldata[idx,]
testing <- modeldata[-idx,]
control <- trainControl(method="cv", 5)
model <- train(classe ~ ., data=training, method="rf", trControl=control, ntree=250)
model
predict <- predict(model, testing)
confusionMatrix(testing$classe, predict)
accuracy <- postResample(predict, testing$classe)
accuracy
result <- predict(model, training[, -length(names(training))])
result
treeModel <- rpart(classe ~ ., data=cleantraining, method="class")
prp(treeModel)
library(e1071)
install.packages("e1071")
library(e1071)
control <- trainControl(method="cv", 5)
model <- train(classe ~ ., data=training, method="rf", trControl=control, ntree=250)
control <- trainControl(method="cv", 5)
model <- train(classe ~ ., data=training, method="rf", trControl=control, ntree=250)
testing_data <- testing_data[featureset[featureset!='classe']]
answers <- predict(model, newdata=testing_data)
answers
str(control)
model <- train(classe ~ ., data=training, method="rf", trControl=control, ntree=250)
model
predict <- predict(model, testing)
confusionMatrix(testing$classe, predict)
accuracy <- postResample(predict, testing$classe)
accuracy
result <- predict(model, training[, -length(names(training))])
result
treeModel <- rpart(classe ~ ., data=cleantraining, method="class")
prp(treeModel)
##############################
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
testing_data <- testing_data[featureset[featureset!='classe']]
answers <- predict(model, newdata=testing_data)
answers
pml_write_files(answers)
head(featureset)
head(modeldata)
library(rattle)
fancyRpartPlot(treeModel)
install.packages("rattle")
library(rattle)
fancyRpartPlot(treeModel)
## Overview
The purpose of this project is to create a prediction algoritm that indicates if a person is performing an arm curl correctly. The data  can be found [here](http://groupware.les.inf.puc-rio.br/har).
```{r, echo=FALSE}
setwd("C:/Users/ldl7766/Desktop/Coursera/A. Data Science Specialization/8. Practical Machine Learning/Project")
```
## Load Packages
```{r, warning=FALSE}
library(data.table)
library(caret)
library(randomForest)
library(foreach)
library(rpart)
library(rpart.plot)
library(corrplot)
library(e1071)
```
## Load Data
training <- read.csv("pml-training.csv", na.strings=c("#DIV/0!"," ", "", "NA", "NAs", "NULL"))
testing <- read.csv("pml-testing.csv", na.strings=c("#DIV/0!"," ", "", "NA", "NAs", "NULL"))
#Remove non-important variables
training <- training[, -which(names(training) %in% c("X", "user_name", "raw_timestamp_part_1", "raw_timestamp_part_2", "cvtd_timestamp", "new_window", "num_window"))]
training = training[, colSums(is.na(training)) == 0] #Remove variables with missing data
novariance =nearZeroVar(training[sapply(training, is.numeric)], saveMetrics=TRUE)  #Identify variables with no variance
training = training[, novariance[, 'nzv'] == 0]
head(training)
inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE )
training_data <- training[inTrain,]
testing_data <- training[-inTrain,]
modFit <- train(classe ~ ., data=training_data, method="rf", prox=TRUE)
modFit <- train(classe ~ ., data=training_data, method="rf", prox=TRUE, ntree=100)
modFit <- train(classe ~ ., data=training_data, method="rpart")
print(modFit$finalModel)
modFit
predict <- predict(modFit, testing_data)
confusionMatrix(testing_data$classe, predict)
fancyRpartPlot(modFit$finalModel)
